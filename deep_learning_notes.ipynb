{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff7f789",
   "metadata": {},
   "source": [
    "# DEEP LEARNING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d480bef",
   "metadata": {},
   "source": [
    "## Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8920914",
   "metadata": {},
   "source": [
    "正规方程指对于损失函数$E = E(\\vec {\\hat y},\\vec y)$, 其中$\\vec {\\hat y} = \\vec f(\\vec w,X)$为预测函数. $\\vec y$为标签值. 利用向量函数梯度为零, 反解出$\\vec w$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b233b",
   "metadata": {},
   "source": [
    " *求解如下:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d796f",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E}{\\partial \\vec w} = \\frac{\\partial E}{\\partial \\vec{\\hat y}} \\frac{\\partial \\vec{\\hat y}}{\\partial \\vec w} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02987d34",
   "metadata": {},
   "source": [
    "如果能显式的求得$\\vec w = \\vec g(X, \\vec y)$, 则这样的$\\vec w$称作正规方程. 此处的$X$为设计矩阵."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31b983",
   "metadata": {},
   "source": [
    "表示数据集的常用方法是 设计矩阵（design matrix）. 设计矩阵的每一行包含\n",
    "一个不同的样本, 每一列对应不同的特征."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaecb9",
   "metadata": {},
   "source": [
    "# 参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c911a",
   "metadata": {},
   "source": [
    "在两层线性神经网络层中, 设前层含有的神经元个数为$n_1$后一层为$n_2$, 那么全连接参数量为$n_1 \\times n_2$, 这个参数可以写作一个矩阵, 维度为$n_1\\times n_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae7e0a",
   "metadata": {},
   "source": [
    "***Instance***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2d946",
   "metadata": {},
   "source": [
    "对于每一个样本, 样本属性空间是n维的, 即$\\vec x \\in \\mathbb{R}^n$, 则属性权重$\\vec w \\in \\mathbb{R}^n$, 预测值为标量$\\hat y$, 则有\n",
    "$$\\hat y = (\\vec w^T)_{1\\times n} \\cdot \\vec {x}_{n\\times 1} = (\\vec x^T)_{1\\times n}\\cdot \\vec w_{n\\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1b3b6",
   "metadata": {},
   "source": [
    "m个样本有\n",
    "$$\\hat y_1 =(\\vec x_1^T)_{1\\times n}\\cdot \\vec w_{n\\times 1} \\\\\n",
    "\\hat y_2 = (\\vec x_2^T)_{1\\times n}\\cdot \\vec w_{n\\times 1}\\\\\n",
    "\\vdots \\\\\n",
    "\\hat y_m = (\\vec x_m^T)_{1\\times n}\\cdot \\vec w_{n\\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eead13",
   "metadata": {},
   "source": [
    "整合为\n",
    "$$\\vec {\\hat y}_{m\\times 1} = X_{m\\times n} \\cdot \\vec w_{n \\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f209a8",
   "metadata": {},
   "source": [
    "$X$为设计矩阵, 考虑到bias的影响, 可使样本属性空间$\\vec x$变成n+1维$\\mathbb{R}^{n+1}$, 增加的维度属性分量始终为1, $\\vec w$同样扩展一维."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2718492",
   "metadata": {},
   "source": [
    "藉此, 可求解正规方程. 设损失函数为$$MSE = \\frac{1}{m}(\\vec{\\hat y} - \\vec y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eab62d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "MSE &= \\frac{1}{m}(\\vec{\\hat y} - \\vec y)^2\\\\\n",
    " &= \\frac{1}{m}(X\\vec w - \\vec y)^2 \\\\\n",
    " &= \\frac{1}{m}(X\\vec w - \\vec y)^T(X\\vec w- \\vec y)\\\\\n",
    " &= \\vec w^TX^TX\\vec w - 2(X\\vec w)^T\\vec y + \\vec y^T\\vec y\n",
    " \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984bc42",
   "metadata": {},
   "source": [
    "对$\\vec w$求偏导并令之为零有\n",
    "$$\n",
    "2X^TX\\vec w - 2X^T\\vec y = 0\n",
    "$$\n",
    "解得\n",
    "$$\\vec w = (X^TX)^{-1}X^T\\vec y$$\n",
    "则此时$\\vec w的表达式即为正规方程$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c39a40",
   "metadata": {},
   "source": [
    "**前馈网络**\n",
    "\n",
    "对于一个一般的模型需要设计每一层的神经网络元数, 设第一层神经元个数(attribute数量)为$m$, 隐藏层神经元个数(attribute个数)为$n$, 输出层神经元个数为$s$(预测每个分类类别对应的的概率), 并且输入的数据量为$a$个, 非线性激活函数$S$, $F$为对应个数输入的预测输出矩阵, 设计函数为:\n",
    "$$F_{a \\times s} = S(X_{a \\times n}W_{n\\times m} + C_{a\\times m})_{a \\times m}W^{\\prime}_{m\\times s} + B_{a\\times s}$$\n",
    "事实上, 隐藏层将具有$n$个特征的输入映射到了具有$m$个特征的输入, 通过非线性激活, 可以对其加入非线性特征. $W$即为从$n$维特征空间到$m$维特征空间的变化矩阵. 整个$F$外可加函数处理最终的值\n",
    "\n",
    "右乘输入矩阵改变特征空间维数, 左乘输入矩阵改变输入元的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49143d",
   "metadata": {},
   "source": [
    "很多输出单元都会包含一个指数函数，这在它的变量取绝对值非常大的负值时会造成饱和。负对数似然代价函数中的对数函数消除了某些输出单元中的指数效果。我们将会在第 6.2.2 节中讨论代价函数和输出单元的选择间的相互作用\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6bcb3",
   "metadata": {},
   "source": [
    "通过将损失函数写成 softplus 函数的形式，我们可以看到它仅仅在 (1 − 2y)z 取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时——当 y = 1 且 z 取非常大的正值时，或者y = 0 且 z 取非常小的负值时。当 z 的符号错误时，softplus 函数的变量 (1 − 2y)z可以简化为 |z|。当 |z| 变得很大并且 z 的符号错误时，softplus 函数渐近地趋向于它的变量 |z|。对 z 求导则渐近地趋向于 sign(z)，所以，对于极限情况下极度不正确的z，softplus 函数完全不会收缩梯度。这个性质很有用，因为它意味着基于梯度的学习可以很快地改正错误的 z。\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345b6e",
   "metadata": {},
   "source": [
    "*使用sigmoid或者softmax等含有exp的函数, 最好使用对数似然*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62300aad",
   "metadata": {},
   "source": [
    "因此，最大似然几乎总是训练 sigmoid 输出单元的优选方法。\n",
    "---\n",
    "在软件实现时，为了\n",
    "避免数值问题，最好将负的对数似然写作 z 的函数，而不是 yˆ = σ(z) 的函数。如\n",
    "果 sigmoid 函数下溢到零，那么之后对 yˆ 取对数会得到负无穷。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b7951",
   "metadata": {},
   "source": [
    "线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f2e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
